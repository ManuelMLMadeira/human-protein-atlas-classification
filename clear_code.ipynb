{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3fd4b30584e8fdf544f23d1a56028fa5fa05d1f"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "SEED = 777\n",
    "SHAPE = (512, 512, 4)\n",
    "DIR = '.'\n",
    "ia.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd1045b817e77ff540e789ea3b0eea22bcdb9a6f"
   },
   "outputs": [],
   "source": [
    "def getTrainDataset():\n",
    "    \n",
    "    path_to_train = DIR + '/train/'\n",
    "    data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "        y = np.zeros(28)\n",
    "        for key in lbl:\n",
    "            y[int(key)] = 1\n",
    "        paths.append(os.path.join(path_to_train, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "def getTestDataset():\n",
    "    \n",
    "    path_to_test = DIR + '/test/'\n",
    "    data = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name in data['Id']:\n",
    "        y = np.ones(28)\n",
    "        paths.append(os.path.join(path_to_test, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# credits: https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n",
    "# credits: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "# Since our data set has samples which are really heavy, here we implement a custom data generator for fast \n",
    "# on-fly data loading. With cache = True, we let the Data Generator save all the data into RAM and then use \n",
    "# these in the following epochs.\n",
    "\n",
    "class ProteinDataGenerator(keras.utils.Sequence):\n",
    "            \n",
    "    def __init__(self, paths, labels, batch_size, shape, shuffle = False, use_cache = False):\n",
    "        self.paths, self.labels = paths, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shape = shape\n",
    "        self.shuffle = shuffle\n",
    "        self.use_cache = use_cache\n",
    "        if use_cache == True:\n",
    "            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], shape[2]), dtype=np.float16)\n",
    "            self.is_cached = np.zeros((paths.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        paths = self.paths[indexes]\n",
    "        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n",
    "        \n",
    "        # Generate data\n",
    "        if self.use_cache == True:\n",
    "            X = self.cache[indexes]\n",
    "            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n",
    "                image = self.__load_image(path)\n",
    "                self.is_cached[indexes[i]] = 1\n",
    "                self.cache[indexes[i]] = image\n",
    "                X[i] = image\n",
    "        else:\n",
    "            for i, path in enumerate(paths):\n",
    "                X[i] = self.__load_image(path)\n",
    "\n",
    "        y = self.labels[indexes]\n",
    "                \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "            \n",
    "    def __load_image(self, path):\n",
    "        R = Image.open(path + '_red.png')\n",
    "        G = Image.open(path + '_green.png')\n",
    "        B = Image.open(path + '_blue.png')\n",
    "        Y = Image.open(path + '_yellow.png')\n",
    "\n",
    "        im = np.stack((\n",
    "            np.array(R), \n",
    "            np.array(G), \n",
    "            np.array(B),\n",
    "            np.array(Y)), -1)\n",
    "        \n",
    "        im = cv2.resize(im, (SHAPE[0], SHAPE[1]))\n",
    "        im = np.divide(im, 255)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31aff62537d634ee34cd7752025d98615b1fc8e9"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, Conv2D, MaxPooling2D, BatchNormalization, Concatenate, ReLU, LeakyReLU, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5748b96367ce78ea9bce4a34a40a66dd5a49e945"
   },
   "outputs": [],
   "source": [
    "# Defining Macro F1 Score compatible with Keras\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "# Defining Macro F1 Score loss function, accordingly to what is described on the report, compatible with Keras\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1-K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d9bbc05fdf72242f8f98422ef9c480a353f4fe3"
   },
   "outputs": [],
   "source": [
    "# Our model, based on GapNet\n",
    "\n",
    "def create_model(input_shape):\n",
    "    \n",
    "    dropRate = 0.4\n",
    "    \n",
    "    init = Input(input_shape)\n",
    "    x = BatchNormalization(axis=-1)(init)\n",
    "    x = Conv2D(32, (3, 3))(x) #, strides=(2,2))(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    ginp1 = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(ginp1)\n",
    "    x = Conv2D(64, (3, 3), strides=(2,2))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    ginp2 = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(ginp2)\n",
    "    x = Conv2D(128, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(128, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(128, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    ginp3 = Dropout(dropRate)(x)\n",
    "    \n",
    "    gap1 = GlobalAveragePooling2D()(ginp1)\n",
    "    gap2 = GlobalAveragePooling2D()(ginp2)\n",
    "    gap3 = GlobalAveragePooling2D()(ginp3)\n",
    "    \n",
    "    x = Concatenate()([gap1, gap2, gap3])\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(28)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    model = Model(init, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4982e06ba3bb236e2bb10b0b08683c93149a61c"
   },
   "outputs": [],
   "source": [
    "model = create_model(SHAPE)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-03),\n",
    "    metrics=['acc',f1])\n",
    "\n",
    "\n",
    "model.save('./BEST_kFold/0')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8831e442b8f5461dfc37da4292f66e1684c84c8"
   },
   "outputs": [],
   "source": [
    "# TRAINING + K-FOLD VALIDATION!\n",
    "\n",
    "paths, labels = getTrainDataset()\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "epochs = 50\n",
    "use_multiprocessing = False \n",
    "workers = 1                  \n",
    "\n",
    "k = 3\n",
    "l = int(paths.shape[0] / k)\n",
    "\n",
    "for i in range(k):\n",
    "\n",
    "    pathsTrain = np.concatenate(paths[:i*l], paths[(i+1)*l:])\n",
    "    labelsTrain = np.concatenate([labels[:i*l], labels[(i+1)*l:]])\n",
    "    pathsVal = paths[i*l:(i+1)*l]\n",
    "    labelsVal = labels[i*l:(i+1)*l]\n",
    "    \n",
    "    print (\"Generated Dimensions:\")\n",
    "    print(paths.shape, labels.shape)\n",
    "    print(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n",
    "\n",
    "    tg = ProteinDataGenerator(pathsTrain, labelsTrain, BATCH_SIZE, SHAPE, use_cache=False, shuffle = False)\n",
    "    vg = ProteinDataGenerator(pathsVal, labelsVal, BATCH_SIZE, SHAPE, use_cache=False, shuffle = False)\n",
    "    \n",
    "    #Raw model from cell above\n",
    "    model = load_model ('./BEST_kFold/0')\n",
    "    \n",
    "    directory = './BEST_kFold/{epoch:02d}-{val_loss:.6f}-{loss:.6f}-i:' + str(i)\n",
    "    checkpoint = ModelCheckpoint(directory, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='min', period=1)\n",
    "    reduceLROnPlato = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min')\n",
    "    \n",
    "    hist = model.fit_generator(\n",
    "    tg,\n",
    "    steps_per_epoch=len(tg),\n",
    "    validation_data=vg,\n",
    "    validation_steps=8,\n",
    "    epochs=epochs,\n",
    "    use_multiprocessing=use_multiprocessing,\n",
    "    workers=workers,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint, reduceLROnPlato])\n",
    "    \n",
    "\n",
    "    # Plotting loss and accuracy over epochs\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].set_title('Loss Over Epochs')\n",
    "    ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "    ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    ax[1].set_title('Accuracy Over Epochs')\n",
    "    ax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\n",
    "    ax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    path_fig = './BEST_kFold/plots' + ' - i:' + str(k) + '.png'\n",
    "    fig.savefig(path_fig)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe7fe7be3814319d6ccace15f05e4477e239c71f"
   },
   "outputs": [],
   "source": [
    "# FINE-TUNING!\n",
    "\n",
    "# Choose which model to fine-tune:\n",
    "filepath = ########\n",
    "model = load_model(filepath, custom_objects={'f1': f1}) # in case you used the f1_loss, you should add\n",
    "                                                        # , 'f1_loss': f1_loss}) there\n",
    "\n",
    "savepath = #####\n",
    "checkpoint2 = ModelCheckpoint(savepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='min', period=1)\n",
    "reduceLROnPlato2 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min')\n",
    "\n",
    "epochs = 60\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "model.layers[-5].trainable = True\n",
    "model.layers[-6].trainable = True\n",
    "model.layers[-7].trainable = True\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])\n",
    "\n",
    "#define tg and vg again, if necessary!\n",
    "\n",
    "hist = model.fit_generator(\n",
    "    tg,\n",
    "    steps_per_epoch=len(tg),\n",
    "    validation_data=vg,\n",
    "    validation_steps=8,\n",
    "    epochs=epochs,\n",
    "    use_multiprocessing=False, \n",
    "    workers=1,\n",
    "    verbose=1,\n",
    "    max_queue_size=4,\n",
    "    callbacks=[checkpoint2, reduceLROnPlato2]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('Loss Over Epochs')\n",
    "ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('Accuracy Over Epochs')\n",
    "ax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "fig.savefig(path_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe7fe7be3814319d6ccace15f05e4477e239c71f"
   },
   "outputs": [],
   "source": [
    "# Preparing results \n",
    "\n",
    "# Load model after pure training (no finetuning)\n",
    "bestModel = load_model(####, custom_objects={'f1': f1}) #, 'f1_loss': f1_loss})\n",
    "\n",
    "# Load model after finetuning    \n",
    "model = load_model(####, custom_objects={'f1': f1}) #, 'f1_loss': f1_loss})\n",
    "\n",
    "# Use the whole validation set (or the set you want)\n",
    "fullValGen = vg\n",
    "\n",
    "from sklearn.metrics import f1_score as off1\n",
    "\n",
    "# Obtain the threshold (binary classification problem for each class) for each class that maximizes its f1_score on \n",
    "# the validation set. T is the array where each entry corresponds to the best threshold for that class. It is obtained \n",
    "# by increasing by 0.001 from 0 to 1 its value and calculating the f1_score for each class, saving the best one. \n",
    "# Besides returning this array T, it still returns the macro f1_score for the validation set using those thresholds.\n",
    "def getOptimalT(mdl, fullValGen):\n",
    "    \n",
    "    lastFullValPred = np.empty((0, 28))\n",
    "    lastFullValLabels = np.empty((0, 28))\n",
    "    for i in tqdm(range(len(fullValGen))): \n",
    "        im, lbl = fullValGen[i]\n",
    "        scores = mdl.predict(im)\n",
    "        lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
    "        lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
    "    print(lastFullValPred.shape, lastFullValLabels.shape)\n",
    "    \n",
    "    rng = np.arange(0, 1, 0.001)\n",
    "    f1s = np.zeros((rng.shape[0], 28))\n",
    "    for j,t in enumerate(tqdm(rng)):\n",
    "        for i in range(28):\n",
    "            p = np.array(lastFullValPred[:,i]>t, dtype=np.int8)\n",
    "            scoref1 = off1(lastFullValLabels[:,i], p, average='binary')\n",
    "            f1s[j,i] = scoref1\n",
    "    \n",
    "    plt.plot(rng, f1s)\n",
    "    T = np.empty(28)\n",
    "    for i in range(28):\n",
    "        T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\n",
    "    \n",
    "    #in order to know the distribution of f1_scores per class\n",
    "    f1_score_p_class = np.max(f1s, axis=0)\n",
    "    print('f1_score for each class is:')\n",
    "    print(f1_score_p_class)\n",
    "    \n",
    "    return T, np.mean(f1_score_p_class)\n",
    "\n",
    "# Calculate T and macro f1_score for each of the chosen models\n",
    "print('Best pure training (no finetuning) model:')\n",
    "T2, ff2 = getOptimalT(bestModel, fullValGen)\n",
    "\n",
    "print('Last model after fine-tuning:')\n",
    "T1, ff1 = getOptimalT(model, fullValGen)\n",
    "\n",
    "print ('raw model - macrof1_score:')\n",
    "print (ff2)\n",
    "print ('model after fine-tuning - macrof1_score:')\n",
    "print (ff1)\n",
    "\n",
    "#We'll use the model with higher macro f1_score on the validation set\n",
    "if ff1 > ff2:\n",
    "    T = T1\n",
    "    BEST = model\n",
    "else:\n",
    "    T = T2\n",
    "    BEST = bestModel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe7fe7be3814319d6ccace15f05e4477e239c71f"
   },
   "outputs": [],
   "source": [
    "#Prediction on the test set\n",
    "pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "testg = ProteinDataGenerator(pathsTest, labelsTest, BATCH_SIZE, SHAPE)\n",
    "submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "P = np.zeros((pathsTest.shape[0], 28))\n",
    "\n",
    "for i in tqdm(range(len(testg))):\n",
    "    images, labels = testg[i]\n",
    "    score = BEST.predict(images)\n",
    "    P[i*BATCH_SIZE:i*BATCH_SIZE+score.shape[0]] = score\n",
    "\n",
    "PP = np.array(P)\n",
    "prediction = []\n",
    "for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "    str_label = ''\n",
    "    \n",
    "    for col in range(PP.shape[1]):\n",
    "        if(PP[row, col] < T[col]):\n",
    "            str_label += ''\n",
    "        else:\n",
    "            str_label += str(col) + ' '\n",
    "    prediction.append(str_label.strip())\n",
    "    \n",
    "submit['Predicted'] = np.array(prediction)\n",
    "submit.to_csv('results_best_100epochs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
